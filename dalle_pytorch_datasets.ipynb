{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dalle-pytorch-datasets.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8H8qQkDfR_3"
      },
      "source": [
        "#@title Setup\n",
        "\n",
        "%pip install dalle-pytorch\n",
        "%pip install wandb\n",
        "\n",
        "%pip install gdown\n",
        "!wget https://github.com/aria2/aria2/releases/download/release-1.35.0/aria2-1.35.0.tar.gz\n",
        "!tar -xf aria2-1.35.0.tar.gz\n",
        "%cd \"aria2-1.35.0\"\n",
        "!./configure && make && make install\n",
        "%cd /content\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1LnVLuJmpXV"
      },
      "source": [
        "#@title Download COCO2017, VirtualGenome, and WIT_380k\n",
        "!wget https://www.dropbox.com/s/qrufasneaduttmc/wit_en_small_380k.tar.gz\n",
        "!wget https://www.dropbox.com/s/dtjjz9cpenmpowr/train2017.zip\n",
        "!wget https://www.dropbox.com/s/30sdd7f9m2nuii2/virtual_genome_captions.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZLfLHjeiZQn"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5UZxD8Dh9-N"
      },
      "source": [
        "# !git clone \"https://github.com/afiaka87/dalle-pytorch-datasets.git\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rovjjP-Sh9pv"
      },
      "source": [
        "image_text_folder = \"/content/output\" #@param\n",
        "import wandb\n",
        "!wandb login"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FagaS4d-5yJk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "rIp1ESmFg1qV"
      },
      "source": [
        "import argparse\n",
        "from random import choice, sample\n",
        "from pathlib import Path\n",
        "\n",
        "# torch\n",
        "\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "# vision imports\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision import transforms as T\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.utils import make_grid, save_image\n",
        "\n",
        "# dalle related classes and utils\n",
        "\n",
        "from dalle_pytorch import OpenAIDiscreteVAE, VQGanVAE1024, DiscreteVAE, DALLE\n",
        "from dalle_pytorch.simple_tokenizer import tokenize, tokenizer, VOCAB_SIZE\n",
        "\n",
        "# argument parsing\n",
        "\n",
        "image_text_folder = \"/content/output\" #@param\n",
        "taming = True #@param \n",
        "\n",
        "# helpers\n",
        "\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "# constants\n",
        "\n",
        "VAE_PATH = None\n",
        "DALLE_PATH = None # 'dalle.pt'\n",
        "RESUME = exists(DALLE_PATH)\n",
        "\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 8\n",
        "LEARNING_RATE = 3e-4\n",
        "GRAD_CLIP_NORM = 0.8\n",
        "\n",
        "MODEL_DIM = 512\n",
        "TEXT_SEQ_LEN = 256\n",
        "DEPTH = 6\n",
        "HEADS = 12\n",
        "DIM_HEAD = 64\n",
        "REVERSIBLE = False\n",
        "\n",
        "# reconstitute vae\n",
        "\n",
        "if RESUME:\n",
        "    dalle_path = Path(DALLE_PATH)\n",
        "    assert dalle_path.exists(), 'DALL-E model file does not exist'\n",
        "\n",
        "    loaded_obj = torch.load(str(dalle_path), map_location='cpu')\n",
        "\n",
        "    dalle_params, vae_params, weights = loaded_obj['hparams'], loaded_obj['vae_params'], loaded_obj['weights']\n",
        "\n",
        "    if vae_params is not None:\n",
        "        vae = DiscreteVAE(**vae_params)\n",
        "    else:\n",
        "        vae_klass = OpenAIDiscreteVAE if not taming else VQGanVAE1024\n",
        "        vae = vae_klass()\n",
        "        \n",
        "    dalle_params = dict(        \n",
        "        **dalle_params\n",
        "    )\n",
        "    IMAGE_SIZE = vae.image_size\n",
        "else:\n",
        "    if exists(VAE_PATH):\n",
        "        vae_path = Path(VAE_PATH)\n",
        "        assert vae_path.exists(), 'VAE model file does not exist'\n",
        "\n",
        "        loaded_obj = torch.load(str(vae_path))\n",
        "\n",
        "        vae_params, weights = loaded_obj['hparams'], loaded_obj['weights']\n",
        "\n",
        "        vae = DiscreteVAE(**vae_params)\n",
        "        vae.load_state_dict(weights)\n",
        "    else:\n",
        "        print('using pretrained VAE for encoding images to tokens')\n",
        "        vae_params = None\n",
        "\n",
        "        vae_klass = OpenAIDiscreteVAE if not taming else VQGanVAE1024\n",
        "        vae = vae_klass()\n",
        "\n",
        "    IMAGE_SIZE = vae.image_size\n",
        "\n",
        "    dalle_params = dict(\n",
        "        num_text_tokens = VOCAB_SIZE,\n",
        "        text_seq_len = TEXT_SEQ_LEN,\n",
        "        dim = MODEL_DIM,\n",
        "        depth = DEPTH,\n",
        "        heads = HEADS,\n",
        "        dim_head = DIM_HEAD,\n",
        "        reversible = REVERSIBLE,\n",
        "        attn_types = ('axial_row', 'axial_col', 'conv_like')\n",
        "    )\n",
        "\n",
        "# helpers\n",
        "\n",
        "def save_model(path):\n",
        "    save_obj = {\n",
        "        'hparams': dalle_params,\n",
        "        'vae_params': vae_params,\n",
        "        'weights': dalle.state_dict()\n",
        "    }\n",
        "\n",
        "    torch.save(save_obj, path)\n",
        "\n",
        "# dataset loading\n",
        "\n",
        "class TextImageDataset(Dataset):\n",
        "    def __init__(self, folder, text_len = 256, image_size = 128):\n",
        "        super().__init__()\n",
        "        path = Path(folder)\n",
        "\n",
        "        text_files = [*path.glob('**/*.txt')]\n",
        "\n",
        "        image_files = [\n",
        "            *path.glob('**/*.png'),\n",
        "            *path.glob('**/*.jpg'),\n",
        "            *path.glob('**/*.jpeg')\n",
        "        ]\n",
        "\n",
        "        text_files = {t.stem: t for t in text_files}\n",
        "        image_files = {i.stem: i for i in image_files}\n",
        "\n",
        "        keys = (image_files.keys() & text_files.keys())\n",
        "\n",
        "        self.keys = list(keys)\n",
        "        self.text_files = {k: v for k, v in text_files.items() if k in keys}\n",
        "        self.image_files = {k: v for k, v in image_files.items() if k in keys}\n",
        "\n",
        "        self.image_tranform = T.Compose([\n",
        "            T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
        "            T.RandomResizedCrop(image_size, scale = (0.9, 1.), ratio = (1., 1.)),\n",
        "            T.ToTensor()\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.keys)\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        key = self.keys[ind]\n",
        "        text_file = self.text_files[key]\n",
        "        image_file = self.image_files[key]\n",
        "\n",
        "        image = Image.open(image_file)\n",
        "        descriptions = text_file.read_text().split('\\n')\n",
        "        descriptions = list(filter(lambda t: len(t) > 0, descriptions))\n",
        "        description = choice(descriptions)\n",
        "        description = description.replace(\"  \", \" \")\n",
        "\n",
        "        if len(tokenizer.encode(text)) >= 256:\n",
        "          print(f\"Caption at idx {ind} too long. Selecting a hundred words.\")\n",
        "          description = \" \".join(description.replace(\"  \", \" \").split(\" \")[:100])\n",
        "\n",
        "\n",
        "        try:\n",
        "          tokenized_text = tokenize(description).squeeze(0)\n",
        "        except Exception:\n",
        "          print(f\"Tokenized text failed at index {ind}. Returning {ind+1} instead.\")\n",
        "          if ind < self.__len__() - 1:\n",
        "            return self.__getitem__(ind+1)\n",
        "          else:\n",
        "            return self.__getitem__(ind-1)\n",
        "\n",
        "        mask = tokenized_text != 0\n",
        "        image_tensor = self.image_tranform(image)\n",
        "        return tokenized_text, image_tensor, mask\n",
        "\n",
        "# create dataset and dataloader\n",
        "\n",
        "ds = TextImageDataset(\n",
        "    image_text_folder,\n",
        "    text_len = TEXT_SEQ_LEN,\n",
        "    image_size = IMAGE_SIZE\n",
        ")\n",
        "\n",
        "assert len(ds) > 0, 'dataset is empty'\n",
        "print(f'{len(ds)} image-text pairs found for training')\n",
        "\n",
        "dl = DataLoader(ds, batch_size = BATCH_SIZE, shuffle = True, drop_last = True)\n",
        "\n",
        "# initialize DALL-E\n",
        "\n",
        "dalle = DALLE(vae = vae, **dalle_params).cuda()\n",
        "\n",
        "if RESUME:\n",
        "    dalle.load_state_dict(weights)\n",
        "\n",
        "# optimizer\n",
        "\n",
        "opt = Adam(dalle.parameters(), lr = LEARNING_RATE)\n",
        "\n",
        "# experiment tracker\n",
        "\n",
        "import wandb\n",
        "\n",
        "model_config = dict(\n",
        "    depth = DEPTH,\n",
        "    heads = HEADS,\n",
        "    dim_head = DIM_HEAD\n",
        ")\n",
        "\n",
        "run = wandb.init(project = 'dalle-pytorch-datasets', resume = RESUME, config = model_config)\n",
        "\n",
        "# training\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    for i, (text, images, mask) in enumerate(dl):\n",
        "        text, images, mask = map(lambda t: t.cuda(), (text, images, mask))\n",
        "\n",
        "        loss = dalle(text, images, mask = mask, return_loss = True)\n",
        "\n",
        "        loss.backward()\n",
        "        clip_grad_norm_(dalle.parameters(), GRAD_CLIP_NORM)\n",
        "\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "\n",
        "        log = {}\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            print(epoch, i, f'loss - {loss.item()}')\n",
        "\n",
        "            log = {\n",
        "                **log,\n",
        "                'epoch': epoch,\n",
        "                'iter': i,\n",
        "                'loss': loss.item()\n",
        "            }\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            sample_text = text[:1]\n",
        "            token_list = sample_text.masked_select(sample_text != 0).tolist()\n",
        "            decoded_text = tokenizer.decode(token_list)\n",
        "\n",
        "            image = dalle.generate_images(\n",
        "                text[:1],\n",
        "                mask = mask[:1],\n",
        "                filter_thres = 0.9    # topk sampling at 0.9\n",
        "            )\n",
        "\n",
        "            save_model(f'./dalle.pt')\n",
        "            wandb.save(f'./dalle.pt')\n",
        "\n",
        "            log = {\n",
        "                **log,\n",
        "                'image': wandb.Image(image, caption = decoded_text)\n",
        "            }\n",
        "\n",
        "        wandb.log(log)\n",
        "\n",
        "    # save trained model to wandb as an artifact every epoch's end\n",
        "\n",
        "    model_artifact = wandb.Artifact('trained-dalle', type = 'model', metadata = dict(model_config))\n",
        "    model_artifact.add_file('dalle.pt')\n",
        "    run.log_artifact(model_artifact)\n",
        "\n",
        "save_model(f'./dalle-final.pt')\n",
        "wandb.save('./dalle-final.pt')\n",
        "model_artifact = wandb.Artifact('trained-dalle', type = 'model', metadata = dict(model_config))\n",
        "model_artifact.add_file('dalle-final.pt')\n",
        "run.log_artifact(model_artifact)\n",
        "\n",
        "wandb.finish()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}